{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e03bf51",
   "metadata": {},
   "source": [
    "Encoder Decoder stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680bc5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3354014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13c6bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b83f7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module,N):\n",
    "    \"produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58172586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module.\"\n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(-1,keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x-mean) / (std+self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d9c6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab7895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b9013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"A residual connection followed by a layer norm.\"\n",
    "    def __init__(self,size,dropout):\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x,sublayer):\n",
    "        \"apply residual conn to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x))) # pre-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d116a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05226b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"self-attn and feedforward\"\n",
    "    def __init__(self,size, self_attn, feed_forward,dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size,dropout),2)\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c055ec66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,size,self_attn,src_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.size= size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout),3)\n",
    "        \n",
    "    def forward(self,x,memory,src_mask,tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x,m,m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc71898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"core encoder is a stack of N layers.\"\n",
    "    def __init__(self, layer, N):\n",
    "        self.layers = clones(layer,N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5748701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f09dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,layer,N):\n",
    "        self.layers = clones(layer,N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self,x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,memory,src_mask,tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12199d",
   "metadata": {},
   "source": [
    "Attention and Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8142ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"mask out subsequent positions.\"\n",
    "    attn_shape = (1,size,size)\n",
    "    subsequent_mask = torch.tiru(torch.ones(attn_shape),diagonal=1).type(\n",
    "    torch.uint8\n",
    "    )\n",
    "    return subsequent_mask==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9740f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
